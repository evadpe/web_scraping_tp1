# run_players_scraper.py
import sys
import os
import json
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from datetime import datetime

def setup_directories():
    """CrÃ©er les rÃ©pertoires nÃ©cessaires"""
    directories = ['exports', 'logs', 'data']
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"ğŸ“ RÃ©pertoire crÃ©Ã©: {directory}/")

def run_players_scraper():
    """Lance le scraper pour extraire les donnÃ©es des joueurs FFVB"""
    print("ğŸ FFVB PLAYERS SCRAPER - PROJET COMPLET")
    print("=" * 60)
    print("ğŸ¯ Objectif: Extraire les donnÃ©es complÃ¨tes des joueurs FFVB")
    print("ğŸ“„ URL cible:", 'http://www.ffvb.org/index.php?lvlid=384&dsgtypid=37&artid=1217&pos=0')
    print("ğŸ”§ Architecture: Scrapy avec pipelines, middlewares et items")
    print()
    
    # VÃ©rifier la structure du projet
    if not os.path.exists('ffvb_scraper'):
        print("âŒ RÃ©pertoire ffvb_scraper non trouvÃ©")
        print("ğŸ’¡ Assurez-vous d'Ãªtre dans le rÃ©pertoire racine du projet")
        return False
    
    # CrÃ©er les rÃ©pertoires nÃ©cessaires
    setup_directories()
    
    # Ajouter le rÃ©pertoire du projet au PYTHONPATH
    current_dir = os.getcwd()
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)
    
    # Charger les settings du projet
    settings = get_project_settings()
    
    # Override de certains settings pour ce run spÃ©cifique
    settings.update({
        'LOG_FILE': f'logs/ffvb_players_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
        'FEEDS': {
            f'exports/ffvb_players_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json': {
                'format': 'json',
                'encoding': 'utf8',
                'indent': 2,
            }
        }
    })
    
    # CrÃ©er le processus crawler
    process = CrawlerProcess(settings)
    
    try:
        # Importer le spider
        from ffvb_scraper.spiders.ffvb_players_spider import FFVBPlayersSpider
        
        print("ğŸ•·ï¸  DÃ©marrage du scraping des joueurs...")
        print("â³ Temps estimÃ©: 5-15 minutes selon la quantitÃ© de donnÃ©es")
        print("ğŸ“Š Pipelines actives:")
        print("   âœ… Validation des donnÃ©es")
        print("   ğŸš« Filtrage des doublons")
        print("   ğŸ“„ Export CSV (ffvb_players.csv, ffvb_teams.csv, ffvb_staff.csv)")
        print("   ğŸ“‹ Export JSON (ffvb_data_complete.json)")
        print("   ğŸ—„ï¸  Base de donnÃ©es SQLite (ffvb_database.db)")
        print("   ğŸ“Š Statistiques (ffvb_statistics.json)")
        print()
        print("ğŸ”§ Middlewares actifs:")
        print("   ğŸ”„ Rotation des User-Agents")
        print("   â±ï¸  Throttling intelligent")
        print("   ğŸ” Retry avec backoff exponentiel")
        print("   ğŸ“ Logging dÃ©taillÃ©")
        print("   ğŸ›¡ï¸  Gestion d'erreurs avancÃ©e")
        print()
        
        # Lancer le scraping
        process.crawl(FFVBPlayersSpider)
        process.start()
        
        print()
        print("âœ… SCRAPING TERMINÃ‰ AVEC SUCCÃˆS!")
        print()
        
        # Afficher les rÃ©sultats
        display_results()
        
        return True
        
    except ImportError as e:
        print(f"âŒ Erreur d'import: {e}")
        print("ğŸ’¡ VÃ©rifiez que ffvb_players_spider.py est dans ffvb_scraper/spiders/")
        return False
    except Exception as e:
        print(f"âŒ Erreur pendant le scraping: {e}")
        return False

def display_results():
    """Afficher les rÃ©sultats du scraping"""
    print("ğŸ“Š RÃ‰SULTATS DU SCRAPING:")
    print("=" * 40)
    
    # VÃ©rifier les fichiers gÃ©nÃ©rÃ©s
    files_to_check = [
        ('ffvb_players.csv', 'Joueurs'),
        ('ffvb_teams.csv', 'Ã‰quipes'),
        ('ffvb_staff.csv', 'Staff'),
        ('ffvb_data_complete.json', 'DonnÃ©es complÃ¨tes JSON'),
        ('ffvb_database.db', 'Base de donnÃ©es SQLite'),
        ('ffvb_statistics.json', 'Statistiques')
    ]
    
    for filename, description in files_to_check:
        if os.path.exists(filename):
            size = os.path.getsize(filename)
            print(f"âœ… {description}: {filename} ({size:,} bytes)")
            
            # Afficher le nombre de lignes pour les CSV
            if filename.endswith('.csv'):
                try:
                    with open(filename, 'r', encoding='utf-8') as f:
                        lines = sum(1 for line in f) - 1  # -1 pour l'en-tÃªte
                    print(f"   ğŸ“ˆ Nombre d'entrÃ©es: {lines}")
                except:
                    pass
        else:
            print(f"âŒ {description}: {filename} - Non crÃ©Ã©")
    
    # Afficher les statistiques si disponibles
    if os.path.exists('ffvb_statistics.json'):
        try:
            with open('ffvb_statistics.json', 'r', encoding='utf-8') as f:
                stats = json.load(f)
            
            print()
            print("ğŸ“ˆ STATISTIQUES DÃ‰TAILLÃ‰ES:")
            print(f"   ğŸ‘¥ Total joueurs: {stats.get('total_players', 0)}")
            print(f"   ğŸ† Total Ã©quipes: {stats.get('total_teams', 0)}")
            print(f"   ğŸ‘” Total staff: {stats.get('total_staff', 0)}")
            
            if stats.get('average_height'):
                print(f"   ğŸ“ Taille moyenne: {stats['average_height']}cm")
            
            if stats.get('clubs'):
                print(f"   ğŸ›ï¸ Clubs diffÃ©rents: {len(stats['clubs'])}")
            
            # Top 3 des postes
            if stats.get('players_by_position'):
                print("   ğŸ¯ Top 3 des postes:")
                sorted_positions = sorted(stats['players_by_position'].items(), 
                                        key=lambda x: x[1], reverse=True)
                for i, (poste, count) in enumerate(sorted_positions[:3], 1):
                    print(f"      {i}. {poste}: {count} joueurs")
                    
        except Exception as e:
            print(f"âŒ Erreur lecture statistiques: {e}")
    
    print()
    print("ğŸ‰ PROJET SCRAPY TERMINÃ‰!")
    print("ğŸ’¡ Prochaines Ã©tapes:")
    print("   1. ğŸ“Š Analysez ffvb_players.csv dans Excel/LibreOffice")
    print("   2. ğŸ—„ï¸  Explorez ffvb_database.db avec DB Browser for SQLite")
    print("   3. ğŸ“ˆ CrÃ©ez des visualisations avec les statistiques")
    print("   4. ğŸ“ RÃ©digez votre rapport scolaire")
    print()
    print("ğŸ¯ PLUS-VALUE TECHNIQUE:")
    print("   â€¢ Architecture Scrapy professionnelle")
    print("   â€¢ Middlewares pour rotation UA et gestion erreurs")
    print("   â€¢ Pipelines de validation et dÃ©doublonnage")
    print("   â€¢ Exports multiples (CSV, JSON, SQLite)")
    print("   â€¢ Logging et monitoring complets")
    print("   â€¢ Respect de l'Ã©thique du web scraping")

def check_dependencies():
    """VÃ©rifier que toutes les dÃ©pendances sont installÃ©es"""
    dependencies = [
        'scrapy',
        'itemloaders', 
        'itemadapter'
    ]
    
    missing = []
    for dep in dependencies:
        try:
            __import__(dep)
        except ImportError:
            missing.append(dep)
    
    if missing:
        print("âŒ DÃ©pendances manquantes:")
        for dep in missing:
            print(f"   - {dep}")
        print()
        print("ğŸ’¡ Installez-les avec:")
        print(f"   pip install {' '.join(missing)}")
        return False
    
    return True

def main():
    """Fonction principale"""
    print("ğŸ” VÃ©rification des dÃ©pendances...")
    if not check_dependencies():
        return
    
    success = run_players_scraper()
    
    if success:
        print()
        print("ğŸŠ SUCCÃˆS COMPLET!")
        print("Votre projet de web scraping avec Scrapy est terminÃ©.")
        print("Tous les fichiers de donnÃ©es sont prÃªts pour l'analyse.")
    else:
        print()
        print("âŒ Ã‰chec du scraping")
        print("Consultez les logs pour plus de dÃ©tails.")

if __name__ == "__main__":
    main()